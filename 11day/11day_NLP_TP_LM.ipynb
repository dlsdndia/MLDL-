{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 자연어 처리( Natural Language Processing, NLP)\n",
    "\n",
    "- 자연어( Natural Language )란 일상 생활에서 사용하는 언어를 말한다.\n",
    "- 자연어 처리( Natural Language Processing, NLP )란 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일을 말한다.\n",
    "- 자연어 처리는 음성 인식, 내용 요약, 번역, 사용자의 감성 분석, 텍스트 분류 작업( 스팸 메일 분류, 뉴스 기사 카테고리 분류등 ), 질의 응답 시스템, 챗봇과 같은 곳에서 사용되는 분야이다.\n",
    "- 자연어 처리는 기계에게 인간의 언어를 이해시킨다는 점에서 인공지능에 있어서 가장 중요한 연구 분야이면서도, 아직 많은 연구가 필요한 분야이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리를 위한 환경 설정\n",
    "\n",
    "- 텍스트 전처리 환경 구성\n",
    "    - 영문 텍스트 전처리 : NLTK 파이썬 패키지 설치\n",
    "    - 한글 텍스트 전처리 : KoNLPy 파이썬 패키지 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 전처리( Text Preprocessing )\n",
    "\n",
    "- 자연어 처리에서 텍스트 전처리( text preprocessing )은 매운 중요한 작업이다.\n",
    "- 텍스트 전처리는 용토에 맞게 텍스트를 사전에 처리하는 작업이다.\n",
    "- 텍스트 전처리를 재대로 처리하지 않으면 자연어 처리 기법 적용이 제대로 동작하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 토큰화( Tokenization )\n",
    "\n",
    "- 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스( corpus ) 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화( tokenization ), 정제( cleaning ), 정규화( normalization ) 하는 일을 수행 하게 된다.\n",
    "- 주어진 코퍼스( corpus, 말뭉치 )에서 토큰( token )이라는 단위로 나누는 작업을 토큰화( tokenization )라 한다.\n",
    "- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 단어 토큰화( word tokenization )\n",
    "\n",
    "- 토큰의 기준을 단어( word )로 하는 경우를 단어 토큰화( word tokenization )라 한다.\n",
    "- 단어( word )는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print( word_tokenize( \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print( WordPunctTokenizer().tokenize( \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print( text_to_word_sequence( \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화에서 고려사항\n",
    "\n",
    "- 토큰화 작업을 단순하게 코퍼스에서 구두점을 제외하고 공백 기분으로 잘라내는 작업이라고 간주할 수 없다.\n",
    "- 토큰화 적업은 섬세한 알고리즘이 필요하다.\n",
    "\n",
    "1) 구두점이나 특수 문자를 단순제외해서는 안 된다.  \n",
    "2) 줄임말과 단어 내에 띄어쓰기가 있는 경우, 사용 용도에 따라서, 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, 토큰화 작업은 줄임말과 단어 내에 띄어쓰기도 하나로 인식할 수 있는 능력도 가져야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준 토큰화 예제\n",
    "\n",
    "- 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙 및 토큰화 결과\n",
    "\n",
    "규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.  \n",
    "규칙 2. doesn't와 같이 어포스트로피( ' )로 \"접어\"가 함께하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Starting a home-based restaurant may an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.tokenize( text ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 문장 토큰화( Sentence Tokenization )\n",
    "\n",
    "- 토큰의 단위가 문장( Sentence )일 때, 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류( sentence segmentation )라고도 한다.\n",
    "- 보통 갖고있는 코퍼스가 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어있지 않을 가능성이 높다. 따라서 사용하고자 하는 용도에 맞게 하기 위해서는 문장 토큰화가 필요할 수 있다.\n",
    "- 보통 문장 단위로 분류할 때, ?나 .(온점)이나 ! 기준으로 문장을 잘라내면 되지 않을까 생각할 수 있지만 반드시 그렇지는 않다.\n",
    "- 그렇기 때문에 사용하는 코퍼스가 어떤 언어인지, 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라서 직접 규칙들을 정의해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a  mountain and almost to the edge of a cliff. he dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a  mountain and almost to the edge of a cliff.', 'he dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "print( sent_tokenize( text ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = 'I am actively looking for Ph.D. students. and you are a \\\n",
    "Ph.D student.'\n",
    "print( sent_tokenize( text ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어에 대한 문장 토큰화 도구\n",
    "\n",
    "- 박상길님 개발 KSS( Korean Sentence Splitter )\n",
    "\n",
    "https://github.com/likejazz/korean-sentence-splitter\n",
    "\n",
    "- KSS를 윈도우 환경에서 사용시 Cython( pip install cython )과 visual studio C++를 설치해 주어야 한다.\n",
    "\n",
    "설치 방법  \n",
    "pip install kss\n",
    "\n",
    "사용 방법  \n",
    "import kss  \n",
    "kss.split_sentence( [ text ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 한국어 토큰화\n",
    "\n",
    "- 영어는 New York과 같은 합성어나 he's와 같은 줄임말에 대한 예외처리만 한다면, 띄어쓰기( whitespace )를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동한다.\n",
    "- 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족하다. 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 즉, 어절 토큰화는 한국어 NLP에서 지양되고 있다.\n",
    "- 단어 토큰화와 어절 토큰화는 같지 않기 때문이다.\n",
    "- 한국어는 영어와는 다른 형태를 가진 언어인 교착어이다. 교착어는 조사, 어미등을 붙여서 말을 만드는 언어를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 한국어는 교착어\n",
    "\n",
    "- 한국어에는 조사가 존재한다. 예로 그( He/Him )라는 주어나 목적어가 들어간 문장이 있다고 할 때, '그'라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙어 사용된다.\n",
    "- 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식되면 자연어 처리가 힘들고 번거로워진다.\n",
    "- 한국어 NLP에서 조사는 분리해줄 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어 토큰화에서는 형태소( morpheme )란 개념을 반드시 이해해야 한다.\n",
    "- 형태소( morpheme )란 뜻을 가진 가장 작은 말의 단위를 의미한다.\n",
    "    - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언( 명사, 대명사, 수사 ), 수식언( 관형사, 부사 ), 감탄사 등이 있다.\n",
    "    - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소, 접사, 어미, 조사, 어간을 말한다.\n",
    "\n",
    "\n",
    "- 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아닌 형태소 토큰화를 수행해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 품사 태깅( Part-of-speech tagging )\n",
    "\n",
    "- 단어의 표기는 같지만, 품사에 따라서 단어의 의미가 달라지기도 한다.\n",
    "- 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 될 수 있다.\n",
    "- 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해 놓기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NLTK / KoNLPy 이용한 영어, 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'I am actively looking for Ph.D. students. and you are a \\\n",
    "Ph.D. student.'\n",
    "print( word_tokenize( text ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRP : 인칭 대명사, VBP : 동사, RB : 부사, VBG : 현재부사, IN : 전치사  \n",
    "NNP : 복수형 명사, CC : 접속사, DT : 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = word_tokenize( text )\n",
    "pos_tag( x ) # 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 형태소( morpheme ) 단위로 형태소 토큰화( morpheme tokenization )를 수행하게 됨을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt # Okt( 구, Twitter 형태소 분석기 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '열심히 휴식한 당신, 이번주에도 열심히 프로젝트를 하자!!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '휴식', '한', '당신', ',', '이번', '주', '에도', '열심히', '프로젝트', '를', '하자', '!!!']\n"
     ]
    }
   ],
   "source": [
    "print( okt.morphs( text ) ) # 형태소 추축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('휴식', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('이번', 'Noun'), ('주', 'Noun'), ('에도', 'Josa'), ('열심히', 'Adverb'), ('프로젝트', 'Noun'), ('를', 'Josa'), ('하자', 'Noun'), ('!!!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print( okt.pos( text ) ) # 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['휴식', '당신', '이번', '주', '프로젝트', '하자']\n"
     ]
    }
   ],
   "source": [
    "print( okt.nouns( text ) ) # 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma # 꼬꼬마 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '휴식', '하', 'ㄴ', '당신', ',', '이번', '주', '에', '도', '열심히', '프로젝트', '를', '하', '자', '!!!']\n"
     ]
    }
   ],
   "source": [
    "print( kkma.morphs( text ) ) # 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('휴식', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('이번', 'NNG'), ('주', 'NNG'), ('에', 'JKM'), ('도', 'JX'), ('열심히', 'MAG'), ('프로젝트', 'NNG'), ('를', 'JKO'), ('하', 'VV'), ('자', 'ECE'), ('!!!', 'SW')]\n"
     ]
    }
   ],
   "source": [
    "print( kkma.pos( text ) ) # 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['휴식', '당신', '이번', '이번주', '주', '프로젝트']\n"
     ]
    }
   ],
   "source": [
    "print( kkma.nouns( text ) ) # 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 정제( Cleaning ) 와 정규화( Normalization )\n",
    "\n",
    "- 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화( tokenization )\n",
    "- 토큰화 작업 전, 후에 텍스트 데이터를 용도에 맞게 정제( cleaning ) 및 정규화( normalization )하는 일이 항상 함께 한다.\n",
    "\n",
    "    - 정제( cleaning ) : 갖고 있는 코퍼스로부터 노이즈 데이터 제거 작업\n",
    "    - 정규화( normalization ) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어 준다.\n",
    "    \n",
    "1. 규칙에 기반한 표기가 다른 단어들의 통합 : 예) 'USA'와'US'의 통합  \n",
    "2. 대, 소문자 통합  \n",
    "3. 불필요한 단어 제거( Removing Unnecessary Words )  \n",
    "    3.1 등장 빈도가 적은 단어( Removing Rare words )  \n",
    "    3.2 길이가 짧은 단어( Removing words with very a short length )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 어간 추출( Stemming ) / 표제어 추출( Lemmatization )\n",
    "\n",
    "- 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법으로 표제어 추출( Lemmatization )과 어간 추출( Stemming )이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 표제어 추출( Lemmatization )\n",
    "\n",
    "- 표제어( Lemma )는 한글로 '표제어' 또는 '기본 사전형 단어' 정도의 의미\n",
    "- 표제어 추출은 단어들로부터 표제어를 찾아가는 과정이다.\n",
    "- 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단한다.\n",
    "- 표제어 추출은 단어의 형태학적 파싱을 먼저 진행 하는 것이다.\n",
    "    - 어간( stem ) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "    - 접사( affix ) : 단어에 추가적인 의미를 주는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # NLTK 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['policy','doing','organization','have','going','love','lives',\n",
    "         'fly','dies','watched','has','starting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "print( [ n.lemmatize( w ) for w in words ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize( 'dies', 'v' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize( 'has', 'v' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize( 'watched', 'v' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 어간 추출( Stemming )\n",
    "\n",
    "- 어간( stem )을 추출하는 작업을 말하며, 어간 추출은 형태학적 분석을 단순화한 버전이라 할 수 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라 할 수 있다.\n",
    "- 어간 추출 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n"
     ]
    }
   ],
   "source": [
    "print( words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "print( [ s.stem( w ) for w in words ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 불용어( Stopword )\n",
    "\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다.\n",
    "- 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말한다.\n",
    "- 불용어는 NLTK에서 제공되는 불용어를 사용할 수도 있고, 사용자가 직접 정의해서 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. NLTK 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words( 'english' )[ :10 ] # NLTK 불용어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = 'Family is not an important thing. It\\'s everything.'\n",
    "stop_words = set( stopwords.words( 'english' ) )\n",
    "word_tokens = word_tokenize( example )\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append( w )\n",
    "        \n",
    "print( word_tokens )\n",
    "print( result )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 한국어에서 불용어 처리\n",
    "\n",
    "- 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있다.\n",
    "- 하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생길수 있다.\n",
    "- 결국 사용자가 직접 불용어 사전을 만들게 되는 경우가 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "example = '고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은게 \\\n",
    "아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.'\n",
    "stop_words = '아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 \\\n",
    "이렇정도로 하면 아니거든'\n",
    "stop_words = stop_words.split( ' ' )\n",
    "word_tokens = word_tokenize( example )\n",
    "\n",
    "result = [ word for word in word_tokens if not word in stop_words ]\n",
    "\n",
    "print( word_tokens )\n",
    "print( result )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 정수 인코딩( Integer Encoding )\n",
    "\n",
    "- 컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있다.\n",
    "- 자연어 처리( NLP )에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있다. 그리고 이러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 정수에 맵핑( Mapping ) 시키는 전처리 작업이 필요할 때가 있다.\n",
    "- 여기서 고유한 정수는 다른 표현으로는 인덱스를 부여 한다는 뜻이다.\n",
    "- 보통 전처리 또는 빈도수가 높은 단어들만 사용하기 위해서 단어에 대한 빈도수를 기준으로 정렬한 뒤에 인덱스를 부여하는 방법을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. dictionary 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a  mountain and almost to the edge of a cliff.', 'he dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to \\\n",
    "himself was driving him crazy. Finally, the barber went up a  mountain \\\n",
    "and almost to the edge of a cliff. he dug a hole in the midst of some \\\n",
    "reeds. He looked about, to mae sure no one was near.\"\n",
    "text = sent_tokenize( text )\n",
    "print( text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'kept', 'word'], ['keeping', 'huge', 'secret', 'driving', 'crazy'], ['finally', 'barber', 'went', 'mountain', 'almost', 'edge', 'cliff'], ['dug', 'hole', 'midst', 'reeds'], ['looked', 'mae', 'sure', 'one', 'near']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "sentences = []\n",
    "stop_words = set( stopwords.words( 'english' ) )\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize( i )\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len( word ) > 2:\n",
    "                result.append( word )\n",
    "                if word not in vocab:\n",
    "                    vocab[ word ] = 0\n",
    "                vocab[ word ] += 1\n",
    "    sentences.append( result )\n",
    "print( sentences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 2, 'kept': 1, 'word': 1, 'keeping': 1, 'huge': 1, 'secret': 1, 'driving': 1, 'crazy': 1, 'finally': 1, 'went': 1, 'mountain': 1, 'almost': 1, 'edge': 1, 'cliff': 1, 'dug': 1, 'hole': 1, 'midst': 1, 'reeds': 1, 'looked': 1, 'mae': 1, 'sure': 1, 'one': 1, 'near': 1}\n"
     ]
    }
   ],
   "source": [
    "print( vocab )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print( vocab[ 'barber' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 2), ('kept', 1), ('word', 1), ('keeping', 1), ('huge', 1), ('secret', 1), ('driving', 1), ('crazy', 1), ('finally', 1), ('went', 1), ('mountain', 1), ('almost', 1), ('edge', 1), ('cliff', 1), ('dug', 1), ('hole', 1), ('midst', 1), ('reeds', 1), ('looked', 1), ('mae', 1), ('sure', 1), ('one', 1), ('near', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은순으로 정렬\n",
    "vocab_sorted = sorted( vocab.items(), key = lambda x: x[ 1 ], \n",
    "                       reverse = True )\n",
    "print( vocab_sorted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'kept': 2, 'word': 3, 'keeping': 4, 'huge': 5, 'secret': 6, 'driving': 7, 'crazy': 8, 'finally': 9, 'went': 10, 'mountain': 11, 'almost': 12, 'edge': 13, 'cliff': 14, 'dug': 15, 'hole': 16, 'midst': 17, 'reeds': 18, 'looked': 19, 'mae': 20, 'sure': 21, 'one': 22, 'near': 23}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 토큰일수록 낮은 정수 인덱스 부여\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for ( word, frequency ) in vocab_sorted:\n",
    "    #if frequency > 1:\n",
    "        i = i + 1\n",
    "        word_to_index[ word ] = i\n",
    "print( word_to_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV : Out-Of-Vocabulary( 단어 집합에 없는 단어)\n",
    "word_to_index[ 'OOV' ] = len( word_to_index ) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6, 7, 8], [9, 1, 10, 11, 12, 13, 14], [15, 16, 17, 18], [19, 20, 21, 22, 23]]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 결과로 맵핑된 문장\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append( word_to_index[ w ] )\n",
    "        except KeyError:\n",
    "            temp.append( word_to_index[ 'OOV' ] )\n",
    "    encoded.append( temp )\n",
    "    \n",
    "print( encoded )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 패딩( Padding )\n",
    "\n",
    "- 자연어 처리( NLP )시 각 문장( 또는 문서 )의 길이가 서로 다를 수 있다. 컴퓨터는 길이가 전부 동일한 문장들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있다.\n",
    "- 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요한 데 이를 패딩 작업이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6, 7, 8], [9, 1, 10, 11, 12, 13, 14], [15, 16, 17, 18], [19, 20, 21, 22, 23]]\n"
     ]
    }
   ],
   "source": [
    "print( encoded )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  1,  2,  3],\n",
       "       [ 0,  0,  4,  5,  6,  7,  8],\n",
       "       [ 9,  1, 10, 11, 12, 13, 14],\n",
       "       [ 0,  0,  0, 15, 16, 17, 18],\n",
       "       [ 0,  0, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩 작업\n",
    "padded = pad_sequences( encoded )\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  0,  0,  0,  0],\n",
       "       [ 4,  5,  6,  7,  8,  0,  0],\n",
       "       [ 9,  1, 10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18,  0,  0,  0],\n",
       "       [19, 20, 21, 22, 23,  0,  0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences( encoded, padding = 'post' )\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 단어 표현 방법( Word Representation )\n",
    "\n",
    "- 자연어 처리를 위해서는 텍스트 전처리후 다양한 형태로 단어를 표현 하는 기법이 제공된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 단어의 표현 방법\n",
    "\n",
    "- 단어의 표현 방법은 국소 표현 방법( Local Representation ) 방법과 분산 표현 방법( Distributed Representation ) 방법으로 나뉜다.\n",
    "- 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법이다.( 이산 표현, Discrete Representation )\n",
    "    - 예) puppy( 강아지 ), cute( 귀여운 ), lovely( 사랑스러운 )라는 단어가 있을 때 1번, 2번, 3번 등과 같은 숫자로 맵핑하여 부여하는 방법\n",
    "- 분산 표현 방법은 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법이다.( 연속 표현, Continuous Representation )\n",
    "    - 예) puppy( 강아지 )라는 단어 근처에는 주로 cute( 귀여운 ), lovely( 사랑스러운 ) 이라는 단어가 자주 등장하므로, puppy라는 단어는 cute, lovely한 느낌이다로 단어를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어 표현 카테고리\n",
    "\n",
    "![Alt text]( wordrepresentation.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 언어 모델( Language Model )\n",
    "\n",
    "- 언어 모델( Language Model )이란 단어 시퀀스( 문장 )에 확률을 할당하는 모델을 말한다.\n",
    "- 어떤 문장들이 있을 때, 컴퓨터가 '이 문장은 적절해!', '이 문장은 말이 안 돼!'라고 사람처럼 판단할 수 있다면, 컴퓨터가 자연어 처리를 잘 한다고 볼 수 있다. 이게 바로 언어 모델이 하는 일이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram 언어 모델( N-gram Language Model )\n",
    "\n",
    "- 카운트에 기반한 통계적 접근을 사용하는 통계적 언어 모델( Statistical Language Model )의 일종이다.\n",
    "- 다른 통계적 언어 모델과는 달리 일부 단어만을 고려하는 접근 방법을 사용한다.\n",
    "- 일부 단어를 몇 개 보는냐를 결정하는데 이것이 n-gram에서의 n의 가지를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram\n",
    "\n",
    "#### 1. 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "\n",
    "- 통계적 언어 모델( SLM )의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점이다.\n",
    "- 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다. 즉 카운트할 수 없을 가능성이 높다.\n",
    "- 하지만 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성이 높다.\n",
    "\n",
    "$$P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|boy})$$\n",
    "\n",
    "- 가령, An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올 확률로 생각해 보면, 갖고있는 코퍼스에 An adorable little boy is가 있을 가능성 보다는 boy is라는 더 짧은 단어 시퀀스가 존재할 가능성이 더 높다.\n",
    "\n",
    "$$P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|little boy})$$\n",
    "\n",
    "- An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카운트해야만 하지만, 단어의 확률을 구하고자 기존 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 거이다. 즉 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 활률이 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. N-gram\n",
    "\n",
    "- 임의의 개수를 정하기 위한 기준을 사용하는 것이 n-gram이다.\n",
    "- n-gram은 n개의 연속적인 단어 나열을 의미한다.\n",
    "- 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주한다.\n",
    "\n",
    "    문장 : An adorable little boy is spreading smiles  \n",
    "  \n",
    "***uni***grams : an, adorable, little, boy, is, spreading, smiles  \n",
    "***bi***grams : an adorable, adorable little, little boy, boy is,  \n",
    "                is spreading, spreading smiles  \n",
    "***tri***grams : an adorable little, adorable little boy,  \n",
    "                 little boy is, boy is spreading, is spreading smiles  \n",
    "***4***-grams : an adorable little boy, adorable little boy is,  \n",
    "                little boy is spreading, boy is spreading smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다.\n",
    "- 위 예에서 4-grams 언어 모델을 사용하는 경우 'spreading' 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개 단어만을 고려한다.\n",
    "\n",
    "![Alt text]( n-gram.png )\n",
    "\n",
    "$$P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}}$$\n",
    "\n",
    "- 만약 갖고 있는 코퍼스에서 boy is spreading가 1000번 등장했다고 하고, boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 가정한다면\n",
    "- boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%이다.\n",
    "- 따라서 확률적 선택에 따라 insults가 더 맞다고 판단하게 된다.\n",
    "\n",
    "$$P(\\text{insults|boy is spreading}) = 0.500$$\n",
    "$$P(\\text{smiles|boy is spreading}) = 0.200$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram 언어 모델의 한계\n",
    "\n",
    "1) 희소 문제( Sparsity Problem )\n",
    "    - 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재한다.\n",
    "    \n",
    "\n",
    "2) n을 선택하는 것은 trade-off 문제\n",
    "    - 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재한다.\n",
    "    - 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있다.\n",
    "    - n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해진다.\n",
    "    - n이 커질수록 모델 사이즈가 커진다는 문제점도 있다.\n",
    "    - n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다.\n",
    "    - 따라서 적절한 n을 선택해야 한다.\n",
    "    \n",
    "***trade-off 문제로 인한 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장***되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
