{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 과적합(Overfitting)을 막는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈이다.\n",
    "- 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터. 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않는다.\n",
    "- 이는 모델이 학습 데이터를 불필요할 정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터 양 늘리기\n",
    "\n",
    "- 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발행할 확률이 늘어난다.\n",
    "- 때문에 데이터 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있다.\n",
    "- 만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증가( Data Augmentation )이라고 한다.\n",
    "- 이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로 데이터를 증식한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 모델의 복잡도 줄이기\n",
    "\n",
    "- 인공 신경망의 복잡도는 은닉층( hidden layer )의 수나 매개 변수의 수 등으로 결정된다.\n",
    "- 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한가지 조치는 인공 신경망의 복잡도를 줄이는 것이다.\n",
    "    - 인공 신경망에서는 모델에 있는 매개 변수들의 수를 모델의 수용력( capacity )이라고 하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 규제( Regularization ) 적용하기\n",
    "\n",
    "- 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다. 그리고 간단한 모델은 적은 수의 매개 변수를 가진 모델을 말한다.\n",
    "- 복잡합 모델을 좀 더 간단하게 하는 방법으로 가중치 규제( Regularization )가 있다.\n",
    "    - L1 규제 : 가중치 W들의 절대값 합계를 비용 함수에 추가한다. L1 노름이라고도 한다.\n",
    "    - L2 규제 : 모든 가중치 W들의 제곱합을 비용 함수에 추가한다. L2 노름이라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda \\mid w \\mid$ 를 더 한 값을 비용 함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\frac{1}{2} \\lambda w^2$를 더 한 값을 비용 함수로 한다. \n",
    "\n",
    "- $\\lambda$는 규제의 강도를 정하는 하이퍼파라미터이다.\n",
    "\n",
    "- $\\lambda$가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 W들의 값이 작아져야 한다는 특징이 있다.\n",
    "- L1 규제를 예로 들면 L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 한다.\n",
    "- 이렇게 되면, 가중치 W의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어 $H( x ) = W_1x_1 + W_2x_2 + W_3x_3 + W_4x_4$라는 수식이 있을 때 여기에 L1 규제를 사용하였더니, $W_3$의 값이 0이 되었다고 하면, 이는 $W_3$ 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 규제는 L1규제와는 달리 가중치들의 제곱을 최소화하므로 W의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 띈다.\n",
    "- L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용하다.\n",
    "- 만약 이런 판단이 필요없다면 L2 규제가 더 잘 작동하므로 L2 규제를 더 권장한다.\n",
    "- 인공 신경망에서는 L2 규제는 가중치 감쇠( weight decay )라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 때로 Regularization을 정규화로 번역하는 참고 도서가 있는 데, 이는 정규화( Normalization )와 혼동될 수 있으므로 규제 또는 정형화라는 번역이 바람직하다.\n",
    "    - 인공 신경망에서 정규화( Normalization )라는 용어가 쓰이는 기법으로는 배치 정규화, 층 정규화 등이다.\n",
    "    - 참고 : 표준화( standardization )\n",
    "        - 각 observation이 평균을 기준으로 어느 정도 떨어져 있는지를 나타낼 때 사용된다.\n",
    "        - 값의 스케일이 다른 두 개의 변수가 있을 때, 이 변수들의 스케일 차이를 제거해 주는 효과가 있다.\n",
    "        - 제로 평균으로부터 각 값들의 분산을 나타낸다.\n",
    "        - 각 요소의 값에서 평균을 뺀 다음 표준편차로 나누어 준다.\n",
    "![Alt text]( standardization.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - 참고 : 정규화( normalization )\n",
    "        - 정규화는 데이터의 범위를 0과 1로 변환하여 데이터 분포를 조정하는 방법이다.\n",
    "        - ( 해당 값 - 최소값 ) / ( 최대값 - 최소값 )\n",
    "![Alt text]( normalization.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 드롭 아웃( Dropout )\n",
    "\n",
    "- 드롭 아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.\n",
    "- 예를 들어 드롭 아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용한다.\n",
    "\n",
    "![Alt text]( dropout.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 드롭 아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다.\n",
    "- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 드롭아웃 모델\n",
    "model = Sequential()\n",
    "model.add( Dense( 256, input_shape = (max_words,), activation = 'relu' ))\n",
    "model.add( Dropout( 0.5 ) ) # 드롭 아웃 추가, 비율 50%\n",
    "model.add( Dense( 128, activation = 'relu' ) )\n",
    "model.add( Dropout( 0.5 ) ) # 드롭 아웃 추가, 비율 50%\n",
    "model.add( Dense( num_classes, activation = 'softmax' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 심층 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기( Gradient )가 점차적으로 작아지는 현상이 발생할 수 있다.\n",
    "- 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다. 이를 기울기 소실( Gradient Vanishing )이라 한다.\n",
    "- 반대의 경우도 있는데, 기울기가 점차 증가하면서 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 한다. 이를 기울기 폭주( Gradient Exploding )이라고 하며, 순환 신경망( Recurrent Neural Network, RNN )에서 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) ReLU와 ReLU 변형들\n",
    "\n",
    "- 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다. \n",
    "- 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생한다.\n",
    "\n",
    "\n",
    "- 기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU 변형 함수인 Leaky ReLU를 사용한다.\n",
    "    - 은닉층에서는 시그모이드 함수 사용을 지향한다.\n",
    "    - Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결한다.\n",
    "    - 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 변형 함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 그래디언트 클리핑( Gradient Clipping )\n",
    "\n",
    "- 그래디언트 클리핑은 기울기 값을 자르는 것을 의미한다.\n",
    "- 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다. 즉, 임계치만큼 크기를 감소시킨다.\n",
    "- 그래디언트 클리핑은 순환 신경망( Recurrent Neural Network, RNN )에 유용하다. RNN은 BPTT에서 시점을 역행하면서 기울기를 구하는데, 이 때 기울기가 너무 커질 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 그래디언트 클리핑\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam( lr = 0.0001, clipnorm = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 초기화( Weight initialization )\n",
    "\n",
    "- 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다.\n",
    "- 즉, 가중치 초기화만 적절히 해줘도 기울기 소실 문제와 같은 문제를 완화시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 세이비어 초기화( Xavier Initialization )\n",
    "\n",
    "논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "- 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안하였다.\n",
    "- 이 초기화 방법은 제안한 사람의 이름을 따서 세이비어 초기화( Xavier Initialization ) 또는 글로럿 초기화( Glorot Initialization )라 한다.\n",
    "- 이 방법은 균등 분포( Uniform Distribution ) 또는 정규 분포( Normal distribution )로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 새운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 층의 뉴런 개수를 ${n}_{in}$, 다음 층의 뉴런 개수를 ${n}_{out}$이라고 할 때\n",
    "- 글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 한다.\n",
    "\n",
    "$$W \\sim Uniform(-\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }}, +\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }}$를 m이라고 하였을 때, -m 과 +m 사이의 균등 분포를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 $\\sigma$가 다음을 만족하도록 한다.\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac { 2 }{ { n }_{ in }+{ n }_{ out } } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막는다.\n",
    "- 하지만 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같이 S자 형태인 함수와 함께 사용할 경우에는 좋은 성능을 발휘하지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않다.\n",
    "- ReLU 함수나 ReLU 변형 함수들을 활성화 함수로 사용할 경우에는 He 초기화( He Initialization )를 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) He 초기화( He Initialization )\n",
    "\n",
    "논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf\n",
    "\n",
    "- He 초기화( He Initialization )는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉜다.\n",
    "- 다만, He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런 수를 반영하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 층의 뉴런 개수를 ${n}_{in}$이라고 할 때\n",
    "- He 초기화는 균등 분포로 초기화 할 경우에는 다음과 같은 균등 분포 범위를 가지도록 한다.\n",
    "\n",
    "$$W\\sim Uniform(- \\sqrt{\\frac { 6 }{ { n }_{ in } } } , \\space\\space + \\sqrt{\\frac { 6 }{ { n }_{ in } } } )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규 분포로 초기화할 경우에는 표준편차 $\\sigma$가 다음을 만족하도록 한다.\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac { 2 }{ { n }_{ in } } }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시그모이드 함수나 하이퍼볼릭 탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적이다.\n",
    "- ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.\n",
    "- ReLU + He 초기화 방법이 좀 더 보편적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 배치 정규화( Batch Normalization )\n",
    "\n",
    "- ReLU 계열의 함수와 He 초기화를 사용하는 것만으로도 어느 정도 기울기 소실과 폭주를 완화시킬수 있지만, 이 두 방법을 사용하더라도 훈련 중에 언제든 다시 발생할 수 있다.\n",
    "- 기울기 소실이나 폭주를 예방하는 또 다른 방법은 배치 정규화( Batch Normalization )이다.\n",
    "- 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 내부 공변량 변화( Internal Covariate Shift )\n",
    "\n",
    "- 내부 공변량 변화란 학습 과정에서 ***층 별로 입력 데이터 분포가 달라지는 현상***을 말한다.\n",
    "- 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생한다.\n",
    "- 배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장한다.\n",
    "    - 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미\n",
    "    - 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 배치 정규화( Batch Normalization )\n",
    "\n",
    "- 배치 정규화는 한 번에 들어오는 배치 단위로 정규화하는 것을 말한다.\n",
    "- 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다.\n",
    "- 배치 정규화를 요약하면 입력에 대해 평균을 0으로 만들고, 정규화를 한다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행한다. 이 때 두 개의 매개변수 $\\gamma$와 $\\beta$를 사용하는데, $\\gamma$는 스케일을 위해 사용하고, $\\beta$는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화의 수식( BN은 배치 정규화를 의미 )\n",
    "\n",
    "Input : 미니 배치 $B = \\{{x}^{(1)}, {x}^{(2)}, ..., {x}^{(m)}\\}$  \n",
    "Output : $y^{(i)} = BN_{\\gamma, \\beta}(x^{(i)})$\n",
    "\n",
    "$$\\mu_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} \\text{ # 미니 배치에 대한 평균 계산}$$\n",
    "\n",
    "$$\\sigma^{2}_{B} \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\mu_{B})^{2}\\text{ # 미니 배치에 대한 분산 계산}$$\n",
    "\n",
    "$$\\hat{x}^{(i)} \\leftarrow \\frac{x^{(i)} - \\mu_{B}}{\\sqrt{\\sigma^{2}_{B}+\\epsilon}}\\text{ # 정규화}$$\n",
    "\n",
    "$$y^{(i)} \\leftarrow \\gamma\\hat{x}^{(i)} + \\beta = BN_{\\gamma, \\beta}(x^{(i)}) \\text{ # 스케일 조정($\\gamma$)과 시프트($\\beta$)를 통한 선형 연산}$$\n",
    "\n",
    "- $m$은 미니 배치에 있는 샘플의 수\n",
    "- $\\mu_{B}$는 미니 배치 $B$에 대한 평균\n",
    "- $\\sigma_{B}$는 미니 배치 $B$에 대한 표준편차\n",
    "- $\\hat{x}^{(i)}$은 평균이 0이고 정규화 된 입력 데이터\n",
    "- $\\epsilon$은 $\\gamma^{2}$가 0일 때, 분모가 0이 되는 것을 막는 작은 양수. 보편적으로 $10^{-5}$\n",
    "- $\\gamma$는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상\n",
    "- $\\beta$는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상\n",
    "- $y^{(i)}$는 스케일과 시프트를 통해 조정한 $BN$의 최종 결과\n",
    "\n",
    "\n",
    "- 배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선된다.\n",
    "- 가중치 초기화에 훨씬 덜 민감해진다.\n",
    "- 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.\n",
    "- 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 낸다. 마치 드롭아웃과 비슷한 효과를 낸다. 물론 드롭아웃과 함께 사용하는 것이 좋다.\n",
    "- 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려진다. 따라서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요하다.\n",
    "- 배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있다. ( https://arxiv.org/pdf/1805.11604.pdf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 배치 정규화의 한계\n",
    "\n",
    "#### 1) 미니 배치 크기에 의존적이다.\n",
    "\n",
    "- 배치 정규화는 너무 작은 배치 크기에서는 잘 작동하지 않는다.\n",
    "- 단적으로 배치 크기를 1로 하게되면 분산은 0이 된다. 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있다.\n",
    "- 배치 정규화를 적용할 때는 미니 배치보다는 크기가 어느 정도 되는 미니 배치에서 하는 것이 좋다.\n",
    "- 배치 정규화는 배치 크기에 의존적인 면이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 순환 신경망( RNN )에 적용하기 어렵다\n",
    "\n",
    "- RNN은 각 시점( time step ) 마다 다른 통계치를 가진다.\n",
    "- 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만든다.\n",
    "- 따라서 배치 크기에도 의존적이지 않고, RNN에도 적용하는 것이 수월한 방법이 층 정규화( Layer Normalization )이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 층 정규화( Layer Normalization )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 시각화\n",
    "    - $m$이 3이고, 특성의 수가 4일 때 배치 정규화\n",
    "    - 미니 배치란 동일한 특성( feature ) 개수들을 가진 다수의 샘플들을 의미\n",
    "    \n",
    "![Alt text]( batch_norm.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 층 정규화 시각화\n",
    "\n",
    "![Alt text]( layer_norm.png )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
